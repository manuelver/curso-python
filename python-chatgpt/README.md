# APUNTES PYTHON Y CHATGPT

![](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExNDY2MDM0MGFjY2EwNGU0NjcwMDFlYzM1M2EzZGJmMWRiMDViOWVmNyZlcD12MV9pbnRlcm5hbF9naWZzX2dpZklkJmN0PWc/iIqmM5tTjmpOB9mpbn/giphy.gif)

## √çndice


[TEMA 1 - Introducci√≥n a ChatGPT y sus aplicaciones](#tema-1---introducci√≥n-a-chatgpt-y-sus-aplicaciones)
- [APUNTES PYTHON Y CHATGPT](#apuntes-python-y-chatgpt)
  - [√çndice](#√≠ndice)
  - [TEMA 1 - Introducci√≥n a ChatGPT y sus aplicaciones](#tema-1---introducci√≥n-a-chatgpt-y-sus-aplicaciones)
    - [1.1. - ¬øQu√© es ChatGPT?](#11---qu√©-es-chatgpt)
    - [1.2. - ¬øC√≥mo funciona?](#12---c√≥mo-funciona)
    - [1.3. - Arquitectura](#13---arquitectura)
    - [1.4. - Casos de uso](#14---casos-de-uso)
  - [TEMA 2 - Configuraci√≥n y Autenticaci√≥n con la API de OpenAI](#tema-2---configuraci√≥n-y-autenticaci√≥n-con-la-api-de-openai)
    - [2.1. - Pasos para autenticarse a ChatGPT](#21---pasos-para-autenticarse-a-chatgpt)
    - [2.2. - Instalar y configurar las bibliotecas para OpenAI](#22---instalar-y-configurar-las-bibliotecas-para-openai)
    - [2.3. - Descripci√≥n y documentaci√≥n de las bibliotecas](#23---descripci√≥n-y-documentaci√≥n-de-las-bibliotecas)
  - [TEMA 3 - Interactuar con ChatGPT usando Python](#tema-3---interactuar-con-chatgpt-usando-python)
    - [3.1. - Realizar peticiones b√°sicas a chatgpt](#31---realizar-peticiones-b√°sicas-a-chatgpt)
  - [3.2. - Personalizaci√≥n de las peticiones de ChatGPT](#32---personalizaci√≥n-de-las-peticiones-de-chatgpt)
    - [3.2.1. - Temperatura (creatividad)](#321---temperatura-creatividad)
    - [3.2.2. - Tokens m√°ximos (largo)](#322---tokens-m√°ximos-largo)
    - [3.2.3. - Cantidad de respuestas](#323---cantidad-de-respuestas)
  - [3.3. - Procesar y analizar las respuestas de chatgpt](#33---procesar-y-analizar-las-respuestas-de-chatgpt)
    - [3.3.1. - Analizar la respuesta](#331---analizar-la-respuesta)
    - [3.3.2. - Utilizar la informaci√≥n extra√≠da](#332---utilizar-la-informaci√≥n-extra√≠da)
  - [TEMA 4 - Aplicaciones Pr√°cticas de Python + ChatGPT](#tema-4---aplicaciones-pr√°cticas-de-python--chatgpt)
    - [4.1. - Chatbot b√°sico](#41---chatbot-b√°sico)
    - [4.2. - Mantener contexto de las conversaciones](#42---mantener-contexto-de-las-conversaciones)
    - [4.3. - Generaci√≥n de contenido y res√∫menes autom√°ticos](#43---generaci√≥n-de-contenido-y-res√∫menes-autom√°ticos)
    - [4.4. - An√°lisis de sentimiento y clasificaciones](#44---an√°lisis-de-sentimiento-y-clasificaciones)
    - [4.5. - Traducci√≥n](#45---traducci√≥n)
  - [TEMA 5 - Otras consideraciones para la integraci√≥n](#tema-5---otras-consideraciones-para-la-integraci√≥n)
    - [5.1. - Filtrar respuestas ‚Äì Palabras prohibidas](#51---filtrar-respuestas--palabras-prohibidas)
    - [5.2. - Verificar respuestas ‚Äì Relevancia](#52---verificar-respuestas--relevancia)
      - [5.2.1. - Calcular similitudes](#521---calcular-similitudes)
      - [5.2.2. - Vectorizar los valores](#522---vectorizar-los-valores)
      - [5.2.3. - Interceptar la respuesta](#523---interceptar-la-respuesta)

## TEMA 1 - Introducci√≥n a ChatGPT y sus aplicaciones
### 1.1. - ¬øQu√© es ChatGPT?

ChatGPT es un modelo de lenguaje desarrollado por OpenAI, basado en la arquitectura GPT-3.5. GPT significa Generatice Pre-trained Tranformer (Transformador Generativo Pre-enternado). Es un sistema de procesamiento del lenguaje natural (NLP) capaz de generar respuestas coherentes y contextualmente relevantes a partir de las entradas de texto que recibe. Utiliza el aprendizaje autom√°tico y est√° entrenado en una amplia variedad de datos textuales para comprender y generar texto en lenguaje natural.

### 1.2. - ¬øC√≥mo funciona?

El funcionamiento de ChatGPT se basa en un enfoque llamado "aprendizaje autom√°tico" o "aprendizaje profundo". El modelo GPT-3.5, en el que se basa ChatGPT, es una red neuronal profunda que ha sido entrenada en una enorme cantidad de datos textuales para aprender patrones y caracter√≠sticas del lenguaje humano.

Cuando se le proporciona una entrada de texto, como una pregunta o una declaraci√≥n, ChatGPT descompone el texto en unidades m√°s peque√±as, como palabras o tokens. Luego, analiza el contexto de las palabras y utiliza esa informaci√≥n para generar una respuesta relevante. El modelo se entrena para predecir la siguiente palabra o el siguiente token en una secuencia de texto, dada la secuencia anterior.

ChatGPT tiene una estructura en capas. Cada capa procesa y extrae informaci√≥n del texto de entrada de manera gradual y compleja. A medida que la informaci√≥n se mueve a trav√©s de las capas, se capturan representaciones cada vez m√°s abstractas y significativas del texto.

Es importante tener en cuenta que ChatGPT se basa en la informaci√≥n con la que ha sido entrenado y no tiene conocimiento directo del mundo real fuera de los datos que se le han proporcionado durante su entrenamiento. Adem√°s, aunque ChatGPT puede generar respuestas coherentes y contextuales, tambi√©n es posible que ocasionalmente genere respuestas incorrectas o no adecuadas, por lo que siempre se debe usar con cautela y verificar la informaci√≥n en caso de duda.

### 1.3. - Arquitectura

La arquitectura subyacente de ChatGPT se basa en la versi√≥n GPT-3.5 de OpenAI, que es una red neuronal de transformador. Un transformador es un tipo de arquitectura de aprendizaje profundo que ha demostrado excelentes resultados en tareas de procesamiento del lenguaje natural.

El transformador consta de m√∫ltiples capas de atenci√≥n y alimentaci√≥n hacia adelante. Cada capa tiene una estructura similar, que incluye una subcapa de atenci√≥n multi-cabeza y una red de alimentaci√≥n hacia adelante.

La subcapa de atenci√≥n multi-cabeza se encarga de analizar la relaci√≥n entre las diferentes palabras en la secuencia de entrada. Utiliza m√∫ltiples "cabezas de atenci√≥n" para capturar diferentes aspectos de la dependencia entre palabras. Esto permite que el modelo entienda mejor las relaciones a largo plazo y el contexto en el texto.

La red de alimentaci√≥n hacia adelante es una capa de redes neuronales completamente conectadas que procesa las representaciones de salida de la capa de atenci√≥n. Ayuda a capturar relaciones no lineales en el texto y mejorar la calidad de las representaciones de las palabras.

En cuanto al funcionamiento, ChatGPT se entrena utilizando un proceso llamado "aprendizaje autodirigido". Durante el entrenamiento, se proporciona al modelo una gran cantidad de datos textuales de diferentes fuentes, como libros, art√≠culos de noticias, p√°ginas web, conversaciones y m√°s. El modelo aprende a predecir la siguiente palabra en una secuencia de texto, dado el contexto anterior. El entrenamiento tiene dos fases:
- Pre-entrenamiento
- Ajuste fino 

[Paper ChatGPT](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) 

![](img/python-chatgpt01.png)

### 1.4. - Casos de uso
- Chatbots y asistentes virtuales. Ejemplo https://my.replika.com/
- Generaci√≥n de contenido. Ejemplo https://www.shortlyai.com/
- Resumen y parafraseo de textos. Ejemplo https://quillbot.com/
- An√°lisis de sentimiento. 
- Traducci√≥n automatica. DeepL utiliza IAs para sus resultados.

## TEMA 2 - Configuraci√≥n y Autenticaci√≥n con la API de OpenAI
### 2.1. - Pasos para autenticarse a ChatGPT


- Paso 1 ‚Äì Registrarse en OpenAI https://platform.openai.com/ 
- Paso 2 ‚Äì Obtener las credenciales API https://platform.openai.com/account/api-keys 
- Paso 3 ‚Äì Almacenarlas de forma segura

Para guardar las contrase√±as de manera segura nos bajamos la librer√≠a python-dotenv:
```shell
pip install python-dotenv
```

Para guardar la API creamos un fichero .env y ponemos una variable en may√∫sculas con la API:
```python
OPENAI_API_KEY=sk-x5ejwlcnwvkcvME_ncjkwceLAcnoewejkINVENTOcnwjl
```

Luego, para llamar la api desde cualquier fichero python tenemos que llamar a la API mediante la librer√≠a python-dotenv. Ejemplo miPrograma.py:
```python
import os
from dotenv import load_dotenv

load_dotenv()

api_key = os.getenv('OPENAI_API_KEY')
```

### 2.2. - Instalar y configurar las bibliotecas para OpenAI

- Paso 1 ‚Äì Instalar la biblioteca de OpenAI
```pyhton
pip install openai
```

- Paso 2 ‚Äì Importarla y configurar la API key
```python
import openai
```

Ahora vamos a configurar la biblioteca openai con la API

Debajo del c√≥digo ponemos:
```python
openai.api_key = api_key
```

- Paso 3 ‚Äì Verificar la conexi√≥n

Nos aseguramos de que est√° configurada la api llamando a los m√≥delos a la biblioteca:
```python
modelos = openai.Model.list()
```

Nos devolver√° todos los modelos disponibles. Para verlo en consola imprimimos la variable:
```python
print(modelos)
```

Ejecutando el programa podemos ver todos los modelos, para aislar los nombres se puede usar el comando jq:
```shell
python3 code/miPrograma.py | jq '.data[].id'
```

[Gu√≠a del comando jq](https://vergaracarmona.es/guia-del-comando-jq/)

![](https://vergaracarmona.es/wp-content/uploads/2022/11/json_everywhere-1024x696.jpg)


### 2.3. - Descripci√≥n y documentaci√≥n de las bibliotecas

**os:** La librer√≠a os proporciona funciones para interactuar con el sistema operativo en Python. Permite realizar tareas relacionadas con el sistema operativo, como acceder a variables de entorno, manipular rutas de archivos y directorios, ejecutar comandos en la l√≠nea de comandos, entre otros. Documentaci√≥n oficial: https://docs.python.org/3/library/os.html 

**openai:** La librer√≠a Python de OpenAI, llamada openai, proporciona una interfaz para acceder a modelos de lenguaje avanzados desarrollados por OpenAI, como GPT-3.5. Puedes utilizar la librer√≠a openai para generar texto, completar oraciones, responder preguntas, traducir texto, entre otros usos relacionados con el procesamiento del lenguaje natural (NLP). Documentaci√≥n oficial: https://platform.openai.com/docs/libraries

**dotenv:** La librer√≠a dotenv es una herramienta popular utilizada para cargar variables de entorno desde archivos de configuraci√≥n en aplicaciones Python. Permite definir variables de entorno en un archivo .env y luego cargar esas variables en el entorno de ejecuci√≥n de la aplicaci√≥n. Esto proporciona una forma conveniente de gestionar configuraciones sensibles y separarlas del c√≥digo fuente. La librer√≠a dotenv es ampliamente utilizada en combinaci√≥n con frameworks como Django o Flask. Documentaci√≥n oficial: https://pypi.org/project/python-dotenv/ 

## TEMA 3 - Interactuar con ChatGPT usando Python
### 3.1. - Realizar peticiones b√°sicas a chatgpt

- Paso 1 ‚Äì Preparar la petici√≥n
- 
Vamos a definir el modelo que queremos utilizar y que pregunta queremos hacer. Por ejemplo, vamos a crear una variable con el modelo y en otra variable ponemos la pregunta, que se llama prompt: 
```python
modelo = "text-davinci-002"
prompt = "¬øCu√°l es la capital de Costa Rica?"
```

- Paso 2 ‚Äì Enviar la petici√≥n

Usaremos la biblioteca openai para almacenar la respuesta:
```python
respuesta = openai.Completion.create(
    engine=modelo,
    prompt=prompt,
    n=1,  # Opcional. N√∫mero de respuestas a devolver
)
```

- Paso 3 ‚Äì Procesar y mostrar la respuesta

Para mostrar la respuesta en consola:
```python
print(respuesta)
```
Si queremos mostrar solo el texto de la respuesta:
```python
print(respuesta.choices[0].text.strip())
```

## 3.2. - Personalizaci√≥n de las peticiones de ChatGPT
### 3.2.1. - Temperatura (creatividad)

Es un parametro que controla como es de aleatoria la respuesta. Cuanto m√°s alta m√°s diversa ser√° la respuesta, se puede poner valores de 0.1 a 1.
```python
temperature=1 
```

### 3.2.2. - Tokens m√°ximos (largo)

Controlas el largo de la respuesta contando los tokens m√°ximos:
```python
max_tokens=100
```

### 3.2.3. - Cantidad de respuestas

Es con el argumento n que ya hab√≠amos usado, pero necesitaremos un loop for para recorrer las respuestas y mostrarlas:
```python
for idx, opcion in enumerate(respuesta.choices):
    texto_generado = opcion.text.strip()
    print(f"Respuesta {idx + 1}: {texto_generado}\n")
```

## 3.3. - Procesar y analizar las respuestas de chatgpt

### 3.3.1. - Analizar la respuesta

Vamos a utilizar bibliotecas python para mejorar el procesamiento del lenguaje natural de la respuesta:

- **SpaCy**: es una biblioteca de procesamiento del lenguaje natural (NLP) de c√≥digo abierto y muy eficiente. Proporciona una amplia gama de funcionalidades para el procesamiento de texto, como el an√°lisis morfol√≥gico, el etiquetado gramatical, el reconocimiento de entidades nombradas, el an√°lisis de dependencias sint√°cticas, la lematizaci√≥n y la extracci√≥n de frases clave. SpaCy se destaca por su rendimiento r√°pido y su capacidad para procesar grandes vol√∫menes de texto de manera eficiente. *Documentaci√≥n*: https://spacy.io/usage

- **TextBlob**: es una biblioteca de procesamiento del lenguaje natural (NLP) basada en NLTK. Ofrece una interfaz sencilla y f√°cil de usar para realizar tareas comunes de procesamiento de texto, como an√°lisis de sentimientos, correcci√≥n ortogr√°fica, tokenizaci√≥n, extracci√≥n de frases, lematizaci√≥n y etiquetado gramatical. TextBlob es conocido por su facilidad de uso y su capacidad para realizar tareas b√°sicas de NLP con pocas l√≠neas de c√≥digo.* Documentaci√≥n*: https://textblob.readthedocs.io/en/dev/ 

- **NLTK (Natural Language Toolkit)**: es una biblioteca popular y ampliamente utilizada para el procesamiento del lenguaje natural en Python. Ofrece una amplia gama de herramientas y recursos para tareas de procesamiento de texto, como tokenizaci√≥n, etiquetado gramatical, lematizaci√≥n, an√°lisis sint√°ctico, an√°lisis de sentimientos, clasificaci√≥n de texto, entre otras. NLTK tambi√©n proporciona acceso a diversos corpus de texto y modelos pre-entrenados, lo que facilita el desarrollo y la experimentaci√≥n en el campo del procesamiento del lenguaje natural. *Documentaci√≥n*: https://www.nltk.org/

Se instalan con pip install <biblioteca> y tendremos que a√±adirlas con import el en c√≥digo.

Escogemos spacy para realizar algunas pruebas.

Se instalan con pip install spacy y tendremos que a√±adirlas con import spacy el en c√≥digo.

Escogemos el modelo en espa√±ol en el c√≥digo con:
```python
modelo_spacy = spacy.load("es_core_news_md")
```
Este modelo tambi√©n lo tenemos que instalar en la terminal
```shell
python3 -m spacy download es_core_news_md
```

Ahora imprimimos el analisis de la respuesta e imprimimos la lista de tokens, de categor√≠as gramaticales y la relaci√≥n de dependencia con su encabezado:
```python
analisis = modelo_spacy(texto_generado)
for token in analisis:
    print(token.text, token.pos_, token.dep_, token.head.text)
```

![](img/python-chatgpt02.png)

Ahora vamos a imprimir las entidades y las etiquetas que los categorizan,
```python
for ent in analisis.ents:
    print(ent.text, ent.label_)
```

![](img/python-chatgpt03.png)

### 3.3.2. - Utilizar la informaci√≥n extra√≠da
Podemos utilizar la informaci√≥n que hemos extra√≠do. Por ejemplo, puede sacar informaci√≥n de tipo LOC, de ubicaci√≥n. Con lo cu√°l se puede generar una pregunta por cada etiqueta de este tipo. Por ejemplo:
```python
ubicacion = None

for ent in analisis.ents:
    # print(ent.text, ent.label_)
    if ent.label_ == "LOC":
        ubicacion = ent
        break

if ubicacion:
    prompt2 = f"Dime m√°s acerca de {ubicacion}"
    respuesta2 = openai.Completion.create(
        engine=modelo,
        prompt=prompt2,
        n=1,
        temperature=1,
        max_tokens=100
    )

    print(respuesta2.choices[0].text.strip())
```

![](img/python-chatgpt04.png)

El fichero del c√≥digo completo es [miPrograma.py](srec/01_miPrograma.py)

## TEMA 4 - Aplicaciones Pr√°cticas de Python + ChatGPT

### 4.1. - Chatbot b√°sico

Lo construimos con un poco de l√≥gica python:
```python
import openai
import os
from dotenv import load_dotenv
load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
openai.api_key = api_key
def preguntar_chat_gpt(prompt, modelo="text-davinci-002"):
    respuesta = openai.Completion.create(
        engine=modelo,
        prompt=prompt,
        n=1,
        temperature=1.5,
        max_tokens=150
    )
    return respuesta.choices[0].text.strip()
# Bienvenida
print("Bienvenido al chatbot de OpenAI GPT-3. \nEscribe \"salir\" cuando quieras terminar la conversaci√≥n.")
# Loop para controlar el flujo de la conversaci√≥n
while True:
    ingreso_usuario = input("\nT√∫: ")
    if ingreso_usuario == "salir":
        break
    prompt = f"Usuario pregunta: {ingreso_usuario}\nChatbot responde: "
    respuesta_gpt = preguntar_chat_gpt(prompt)
    print(f"Chatbot: {respuesta_gpt}")
```

Pero con esto nos encontramos con un problema: No guarda la l√≠nia conversacional. Si nos referimos a una respuesta anterior no la recuerda. Se puede ver en esta conversaci√≥n de besugos:

![](img/python-chatgpt05.png)

### 4.2. - Mantener contexto de las conversaciones

Vamos a crear unas variables para almacenar las conversaciones y crear la funcionalidad necesaria para que cada pregunta se alimente con el historial de la conversaci√≥n.

Las variables antes del loop son:
```python
preguntas_anteriores = []
respuestas_anteriores = []
```

y al principio del loop del control de flujo:
```python
conversacion_historica = ""
```

Dentro del loop creamos la l√≥gica para guardar las preguntas y respuestas:
```python
preguntas_anteriores.append(ingreso_usuario)
respuestas_anteriores.append(respuesta_gpt)
```

Ahora, debemos alimentar a la variable conversaci√≥n hist√≥rica contodo lo que almacenemos. Lo haremos con un loop for:
```python
for pregunta, respuesta in zip(preguntas_anteriores, respuestas_anteriores):
    conversacion_historica += f"Usuario pregunta: {pregunta}\nChatbot responde: {respuesta}\n"
```

Ahora debemos a√±adir en el prompt la conversacion_historica:
```python
conversacion_historica += prompt
```

Entonces, a la respuesta_gpt tenemos que cambiar para que la funci√≥n recoja la converacion_historica. En el print tambi√©n tenemos que dar directamente la respuesta_gpt porque la conversaci√≥n ya guarda el di√°logo. El c√≥digo queda as√≠:
```python
import openai
import os
from dotenv import load_dotenv

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

openai.api_key = api_key

preguntas_anteriores = []
respuestas_anteriores = []


def preguntar_chat_gpt(prompt, modelo="text-davinci-002"):
    """
    Pregunta a la API de OpenAI GPT-3
    """

    respuesta = openai.Completion.create(
        engine=modelo,
        prompt=prompt,
        n=1,
        temperature=1,
        max_tokens=150
    )

    return respuesta.choices[0].text.strip()


# Bienvenida
print("Bienvenido al chatbot de OpenAI GPT-3. \nEscribe \"salir\" cuando quieras terminar la conversaci√≥n.")

# Loop para controlar el flujo de la conversaci√≥n
while True:

    conversacion_historica = ""

    ingreso_usuario = input("\nT√∫: ")

    if ingreso_usuario == "salir":
        break

    for pregunta, respuesta in zip(preguntas_anteriores, respuestas_anteriores):
        conversacion_historica += f"Usuario pregunta: {pregunta}\nChatbot responde: {respuesta}\n"

    prompt = f"Usuario pregunta: {ingreso_usuario}"
    conversacion_historica += prompt
    respuesta_gpt = preguntar_chat_gpt(conversacion_historica)

    print(f"{respuesta_gpt}")

    preguntas_anteriores.append(ingreso_usuario)
    respuestas_anteriores.append(respuesta_gpt)
```

Y el chatbot ya recuerda la conversaci√≥n:

![](img/python-chatgpt06.png)

4.2.1. - Color para diferenciar pregunta y respuesta
Vamos a darle un poco de color para que se vea mejor en la terminal. Lo haremos con la librer√≠a colorama. As√≠ queda el c√≥digo:
```python
import openai
import os
from dotenv import load_dotenv
from colorama import init, Fore

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")

openai.api_key = api_key

preguntas_anteriores = []
respuestas_anteriores = []

# Inicializar colorama
init()


def preguntar_chat_gpt(prompt, modelo="text-davinci-002"):
    """
    Pregunta a la API de OpenAI GPT-3
    """

    respuesta = openai.Completion.create(
        engine=modelo,
        prompt=prompt,
        n=1,
        temperature=0.1,
        max_tokens=150
    )

    return respuesta.choices[0].text.strip()


# Bienvenida
print(Fore.YELLOW + "Bienvenido al chatbot de OpenAI GPT-3." + Fore.RESET)
print(Fore.CYAN + "Escribe \"salir\" cuando quieras terminar la conversaci√≥n." + Fore.RESET)

# Loop para controlar el flujo de la conversaci√≥n
while True:

    conversacion_historica = ""

    ingreso_usuario = input(Fore.MAGENTA + "T√∫: " + Fore.RESET)

    if ingreso_usuario == "salir":
        break

    for pregunta, respuesta in zip(preguntas_anteriores, respuestas_anteriores):
        conversacion_historica += f"{Fore.BLUE}Usuario pregunta: {Fore.RESET}{pregunta}{respuesta}\n"

    prompt = f"{Fore.BLUE}Usuario pregunta: {Fore.RESET}{ingreso_usuario}"
    conversacion_historica += prompt
    respuesta_gpt = preguntar_chat_gpt(conversacion_historica)

    print(f"{Fore.GREEN}{respuesta_gpt}{Fore.RESET}")

    preguntas_anteriores.append(ingreso_usuario)
    respuestas_anteriores.append(respuesta_gpt)
```
El fichero del c√≥digo completo es [chatbot.py](src/02_chatbot.py)
### 4.3. - Generaci√≥n de contenido y res√∫menes autom√°ticos

Vamos a crear dos funciones, una para generar el contenido y otra para resumirlo. Creando un archivo nuevo con las bibliotecas necesarias y cargando la clave de nuevo, como en los anteriores casos.
Funci√≥n generar contenido:
```python
def crear_contenido(tema, tokens, temperatura, modelo="text-davinci-002"):
    prompt = f"Escribe un art√≠culo corto sobre el tema: {tema}"
    respuesta = openai.Completion.create(
        engine=modelo,
        prompt=prompt,
        n=1,
        temperature=temperatura,
        max_tokens=tokens
    )
    return respuesta.choices[0].text.strip()
```

Ahora, vamos a darle la din√°mica al programa:
```python
# Bienvenida
print("Bienvenido a la aplicaci√≥n de creaci√≥n de contenido. \n Necesito que me des algunos datos.")
# Pedir datos
tema = input("Elige un tema para tu art√≠culo: ")
tokens = int(input("Tokens m√°ximos: "))
temperatura = int(
    input("Del 1 al 10, ¬øCu√°nto quieres que sea de creativo el art√≠culo?: ")) / 10
# Crear contenido
articulo_creado = crear_contenido(tema, tokens, temperatura)
print(articulo_creado)
```

![](img/python-chatgpt07.png)

El fichero del c√≥digo completo es [crear_contenido.py](src/03_crear_contenido.py)

Ahora haremos lo mismo en otro fichero con la funci√≥n para el resumen:
```python
def resumir_text(texto, tokens, temperatura, modelo="text-davinci-002
    prompt = f"Resume el siguiente texto: {texto}\n\n"
    respuesta = openai.Completion.create(
        engine=modelo,
        prompt=prompt,
        n=1,
        temperature=temperatura,
        max_tokens=tokens
    )
    return respuesta.choices[0].text.strip()
```
Y su din√°mica:
```python
# Bienvenida
print("Bienvenido a la aplicaci√≥n de creaci√≥n de contenido. \n Necesito que me des algunos datos.")
# Pedir datos
original = input("Pega aqu√≠ el art√≠culo a resumir: ")
tokens = int(input("Tokens m√°ximos: "))
temperatura = int(
    input("Del 1 al 10, ¬øCu√°nto quieres que sea de creativo el resumen?: ")) / 10
# Crear contenido
resumen = resumir_text(original, tokens, temperatura)
print(resumen)
```

Pero en este art√≠culo hay un problema. No podemos pasar saltos de l√≠nea. As√≠ que tenemos que pasar el art√≠culo a texto plano y eliminar todos los saltos de l√≠nea. He usado este art√≠culo: https://www.unicef.org/es/comunicados-prensa/ninos-afectados-por-sequia-zonas-africa-borde-catastrofe 

![](img/python-chatgpt08.png)

El fichero del c√≥digo completo es [resumir_articulo.py](src/04_resumir_articulo.py)

### 4.4. - An√°lisis de sentimiento y clasificaciones

Podemos analizar el sentimiento predominante en el texto y clasificarlo. Todo esto en dos funciones.

La funci√≥n de analisis de texto es:
```python
def analizar_sentimientos(texto):
    prompt = f"Analiza los sentimientos del siguiente texto: '{texto}'. El sentimiento predominante es: "
    respuesta = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        n=1,
        temperature=0.5,
        max_tokens=100
    )
    return respuesta.choices[0].text.strip()
```

Ahora la din√°mica del programa:
```python
texto_para_analizar = input("Pega aqu√≠ el texto a analizar: ")
sentimiento = analizar_sentimientos(texto_para_analizar)
print(sentimiento)
```

![](img/python-chatgpt09.png)


El fichero del c√≥digo completo es [analizar_sentimientos.py](src/05_analizar_sentimientos.py)

Una consideraci√≥n a tener en cuenta es que se podr√≠a crear un programa de scraping que recogiera los comentarios efectuados por usuarios en, por ejemplo, un art√≠culo o un v√≠deo de youtube, y pasarselo a chatgpt para que indicar√° sus conclusiones de sentimiento predominante.

Para clasificar texto la funci√≥n y la din√°mica es:
```python
def clasificar_texto(texto):
    # Definir categor√≠as en un array
    categorias = [
        "Arte",
        "ciencia",
        "deportes",
        "entretenimiento",
        "educaci√≥n",
        "finanzas",
        "historia",
        "literatura",
        "matem√°ticas",
        "medicina",
        "medio ambiente",
        "m√∫sica",
        "noticias",
        "pol√≠tica",
        "religi√≥n",
        "salud",
        "tecnolog√≠a",
        "viajes",
    ]
    prompt = f"Clasifica el siguiente texto: '{texto}' en una de estar categor√≠as: {','.join(categorias)}. La categor√≠a es: "
    respuesta = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        n=1,
        temperature=0.5,
        max_tokens=50
    )
    return respuesta.choices[0].text.strip()
texto_para_clasificar = input("Ingresa texto a clasificar en una categor√≠a: ")
clasificacion = clasificar_texto(texto_para_clasificar)

print(clasificacion)
```

Para la prueba he cogido los comentarios del grupo del canal telegram [seguridadinform√°tic4](t.me/seguridadinformatic4):

![](img/python-chatgpt10.png)


El fichero del c√≥digo completo es [clasificar_texto.py](src/06_clasificar_texto.py)

### 4.5. - Traducci√≥n

Tan solo con una funci√≥n podremos traducir un texto al idioma que queramos. La funci√≥n y la din√°mica del programa:
def traducir_texto(texto, idioma):
```python
    prompt = f"Traduce el siguiente texto al idioma {idioma}:\n\n{texto}\n\nTexto traducido: "
    respuesta = openai.Completion.create(
        engine="text-davinci-002",
        prompt=prompt,
        n=1,
        temperature=0.5,
        max_tokens=100
    )
    return respuesta.choices[0].text.strip()
print("Bienvenido al traductor de texto\n")
idioma = input("Escribe el idioma al que quieres traducir: ")
texto_a_traducir = input("Escribe el texto a traducir: ")
texto_traducido = traducir_texto(texto_a_traducir, idioma)
print(f"El texto traducido es: {texto_traducido}")
```

![](img/python-chatgpt11.png)


El fichero del c√≥digo completo es [traducir_texto.py](src/07_traducir_texto.py)

## TEMA 5 - Otras consideraciones para la integraci√≥n

### 5.1. - Filtrar respuestas ‚Äì Palabras prohibidas

Vamos a recuperar el c√≥digo del chatbot que creamos y le vamos a a√±adir la biblioteca spacy, que nos ayudar√° al procesamiento del lenguaje natural. As√≠ que importamos spacy, a√±adimos la variable cargando el modelo de procesamiento de lenguaje natural y otra de palabras_prohibidas:
```python
import spacy
modelo_spacy = spacy.load("es_core_news_md")
palabras_prohibidas = ["palabra1", "palabra2"]
```

En palabras_prohibidas hemos creado una lista ficticia pero est√°s palabras podr√≠an ser palabras malsonantes, palabras de la competencia, palabras de jerga o cualquier listado que creamos que no debe salir en el output que nos devuelve chatgpt.

Ahora vamos a crear la funci√≥n para el filtrado:
```python
def filtrar_lista_negra(texto, lista_negra):
    token = modelo_spacy(texto)
    resultado = []
    for t in token:
        # Si el token no est√° en la lista negra, agregarlo al resultado
        if t.text not in lista_negra:
            resultado.append(t.text)
        else:
            resultado.append("[xxxxx]")

    return " ".join(resultado)
```

Ahora tenemos que retocar la funci√≥n preguntar_chat_gpt para que llame a la funci√≥n nueva de filtrado en cada respuesta cambiando el return que ten√≠amos:
```python
respuesta_sin_filtrar = respuesta.choices[0].text.strip()
respuesta_filtrada = filtrar_lista_negra(respuesta_sin_filtrar, palabras_prohibidas)
return respuesta_filtrada
```

Antes de hacer la prueba, y para que quede m√°s despejada la ejecuci√≥n del programa en la terminal, vamos a a√±adir una funci√≥n que limpie el terminal con la biblioteca que tenemos importada os:
```python
def clearConsole():
    # Funci√≥n limpiar consola
    os.system('clear')
```

Vamos a llamarla al inicio de la din√°mica del programa, antes de la bienvenida.

Y para probarlo, vamos a a√±adir en las palabras_prohibidas la palabra "paella" y le vamos a preguntar por el plato t√≠pico valenciano:

![](img/python-chatgpt12.png)

### 5.2. - Verificar respuestas ‚Äì Relevancia

Vamos a realizar tres pasos:
- Calcular similitudes
- Vectorizar los valores
- Interceptar la respuesta

#### 5.2.1. - Calcular similitudes

Vamos a seguir con el chatbot para a√±adirle esta funcionalidad. Lo primero es instalar numpy que lo necesitaremos para realizar ciertos calculos. Importamos la biblioteca y le damos un alias:
```python
import numpy as np
```

Y vamos a crear una funci√≥n con algunos principios matem√°ticos para aplicar a nuestro texto. Vamos a hacer que calcule lo que se llama la similitud coseno de dos vectores.
```python
def similitud_coseno(vec1, vec2):
    superposicion = np.dot(vec1, vec2)
    magnitud1 = np.linalg.norm(vec1) # Longitud del vector
    magnitud2 = np.linalg.norm(vec2) # Longitud del vector
    sim_cos = superposicion / (magnitud1 * magnitud2)
    return sim_cos
```

El resultado de esta operaci√≥n va a ser un valor entre -1 y uno. Este valor va a indicar la similitud, o sea, entre los dos vectores.

Si se obtiene un valor uno indica que los vectores son id√©nticos,  que tienen el mismo √°ngulo si hablamos de n√∫meros. Mientras que un valor de -1 indicar√≠a que son completamente opuestos, que su √°ngulo es de 180 grados.
En el caso de los an√°lisis de texto, que es lo que nos interesa a nosotros, los vectores se generan a partir de los textos utilizando un modelo de lenguaje como spacy que tiene la capacidad de convertir cada texto en un vector num√©rico que representa su contenido sem√°ntico, as√≠ como lo escuchas ü§Ø

Con lo cu√°l, la funci√≥n similitud_coseno puede utilizarse para comparar estos vectores que son textos que han sido convertidos a valores num√©ricos seg√∫n su valor sem√°ntico y as√≠ determinar c√≥mo de similares son, en t√©rminos de contenido sem√°ntico.

#### 5.2.2. - Vectorizar los valores

Ahora, vamos a crear otra funci√≥n que recoger√° la respuesta y la entrada para calcular la similitud_coseno:
def es_relevante(respuesta, entrada, umbral=0.5):
```python
entrada_vectorizada = modelo_spacy(entrada).vector
respuesta_vectorizada = modelo_spacy(respuesta).vector
similitud = similitud_coseno(entrada_vectorizada, respuesta_vectorizada)
return similitud >= umbral
```

#### 5.2.3. - Interceptar la respuesta
Ahora ya volvemos al loop que da la din√°mica y l√≥gica al chatbot, justo antes de imprimir la respuesta debemos a√±adir en una variable la llamada a la funci√≥n anterior y el print y la recogida de respuestas meterlas en un condicional si la respuesta el relevante, seg√∫n nuestra funci√≥n. Si es false
```python
relevante = es_relevante(respuesta_gpt, ingreso_usuario)

if relevante:
    print(f"{respuesta_gpt}")

    preguntas_anteriores.append(ingreso_usuario)
    respuestas_anteriores.append(respuesta_gpt)
else:
    print(Fore.RED + "La respuesta no es relevante ¬øpodr√≠as reformularla?" + Fore.RESET)
```

Le he dado colorcito rojo a la respuesta err√≥nea. Vamos a probarlo. Le pido un poema y le pregunto sobre mi opini√≥n:

![](img/python-chatgpt13.png)

Y hasta aqu√≠ el curso. ¬°Sigue cortando le√±a! ü™ì

![](img/python-chatgpt14.png)
